<HTML>
<HEAD>
  <TITLE>Command Line Syntax for the W3C Mini Robot</TITLE>
  <LINK rel=STYLESHEET href="../../StyleSheets/Area/Overview.css" type="text/css">
</HEAD>
<BODY BGCOLOR="#ffffff" TEXT="#000000">
<P>
<A HREF="../"><IMG ALT="W3C" SRC="../../Icons/WWW/w3c_home" BORDER="0" WIDTH="72"
    HEIGHT="48"></A>
<A HREF="../"><IMG ALT="Robot" SRC="../../Icons/WWW/robot48x" WIDTH="48"
    HEIGHT="48" BORDER="0"></A> <IMG ALT="MANUAL" SRC="../../Icons/WWW/doc48x">
<H1>
  Command Line Syntax
</H1>
<P>
<DIV class="intro">
  The generic syntax is:
  <PRE> 	webbot [ <A HREF="#z19">options</A> ] [ <A HREF="#z16">URI</A> [ <A HREF="#z17">keywords</A> ] ]
</PRE>
  <P>
  Check here the following options
  <UL>
    <LI>
      <A HREF="#Help">Getting Help</A>
    <LI>
      <A HREF="#Constraints">Setting Basic Constraints for Traversal</A>
    <LI>
      <A HREF="#txt">Robots.txt and HTML META tags</A>
    <LI>
      <A HREF="#regex">Regular Expressions based Constraints</A>
    <LI>
      <A HREF="#Search">Breath First or Depth First Search</A>
    <LI>
      <A HREF="#Inlined">Checking Inlined Images</A>
    <LI>
      <A HREF="#Logging">SQL Based Logging using MySQL</A>
    <LI>
      <A HREF="#Regular">Regular Ascii Text Logging</A>
    <LI>
      <A HREF="#Stats">Distribution and Statistics Features</A>
    <LI>
      <A HREF="#Persistent">Persistent Cache</A>
    <LI>
      <A HREF="#Other">Other Options</A>
  </UL>
</DIV>
<P>
  <HR>
<H2>
  <A NAME="z19">Options</A>
</H2>
<P>
The order of the options is not important and options can in fact be specified
on either side of any <A HREF="#z16"NAME="z14">URI</A>. Currently available
options are:-
<H3>
  <A NAME="Help">Getting Help</A>
</H3>
<DL>
  <DT>
    <B>-v [ a | b | c | g | p | s | t | u ] </B>
  <DD>
    Verbose mode: Gives a running commentary on the program's attempts to read
    data in various ways. As the amount of verbose output is substantial, the
    <TT>-v</TT> option can now be followed by zero, one or more of the following
    flags (without space) in order to differentiate the verbose output generated:
    <UL>
      <LI>
	<B>a</B>: Anchor relevant information
      <LI>
	<B>b</B>: Bindings to local file system
      <LI>
	<B>c</B>: Cache trace
      <LI>
	<B>g</B>: SGML trace
      <LI>
	<B>p</B>: Protocol module information
      <LI>
	<B>s</B>: SGML/HTML relevant information
      <LI>
	<B>t</B>: Thread trace
      <LI>
	<B>u</B>: URI relevant information
    </UL>
    <P>
    The <TT>-v</TT> option without any appended options shows all trace messages.
    An example is "<TT>-vpt</TT>" showing <EM>thread</EM> and <EM>protocol</EM>
    trace messages
  <DT>
    <B>-version</B>
  <DD>
    Prints out the version number of the robot and the version number of libwww
    and exits.
</DL>
<H3>
  <A NAME="Constraints">Setting Basic Constraints for Traversal</A>
</H3>
<P>
These are some very simple constrants that can always be used when running
the webbot.
<DL>
  <DT>
    <B>-depth [ n ]</B>
  <DD>
    Limit jumps to <B>n</B> hops from the start page. The n-1 link is checked
    using a <TT>HEAD</TT> request. The default value is 0 which means that only
    the start page is searched. A value of 1 will cause the start page <B>and</B>
    all pages directly linked from that start page to be checked.
  <DT>
    <B>-prefix [ URI ]</B>
  <DD>
    Define a URI prefix for all URIs - if they do not match the prefix then they
    are not checked. The rejected URIs can be <A HREF="#Logging">logged to a
    separate file</A>.
</DL>
<H3>
  <A NAME="txt">Robots.txt and HTML META tags</A>
</H3>
<P>
There are situations where you may not want the robot to behave as a robot
but more as a link checker in which case you may consider using these options:
<DL>
  <DT>
    <B>-norobotstxt</B>
  <DD>
    If you for some reason don't want the robot to check for a
    <A HREF="http://info.webcrawler.com/mak/projects/robots/exclusion.html#robotstxt">robots.txt
    file</A> then add this command line option
  <DT>
    <B>-nometatags</B>
  <DD>
    If you for some reason don't want the robot to check for
    <A HREF="http://info.webcrawler.com/mak/projects/robots/exclusion.html#meta">HTML
    robots related META tags</A> then add this command line option
</DL>
<H3>
  <A NAME="regex">Regular Expressions based Constraints</A>
</H3>
<P>
Using regular expressions reguires that you
<A HREF="../../INSTALL.html#features">link against a regex library</A> handling
<A HREF="http://www.delorie.com/gnu/docs/rx/rx_3.html">regular expressions</A>
- see the <A href="../../INSTALL.html">installation instructions</A> for
details. When using regular expressions, you can control the constraints
much more efficiently - both to decide which URIs should be followed and
to decide whether the webbot should use <CODE>HEAD</CODE> or <CODE>GET</CODE>
when checking the links.
<DL>
  <DT>
    <B>-exclude [ regex ] </B>
  <DD>
    Allows you to define a
    <A HREF="http://www.delorie.com/gnu/docs/rx/rx_3.html">regular expression</A>
    of which URIs should be excluded from the traversal. The rejected URIs can
    be <A HREF="#Logging">logged to a separate file</A>. This can be used to
    exclude specific parts of the URI space, for example all URIs containing
    "/old/": <CODE>-exclude "/old/"</CODE>
  <DT>
    <B>-check [ regex ]</B>
  <DD>
    Check all URIs that match this regular expression with a <TT>HEAD</TT> method
    instead of a <TT>GET</TT> method. This can be used to verify links but avoiding
    downloading large distribution files like this: <CODE>-check
    "\.gz$|\.Z$|\.zip$|</CODE>, for example.
  <DT>
    <B>-include [ regex ]</B>
  <DD>
    Allows you to define a
    <A HREF="http://www.delorie.com/gnu/docs/rx/rx_3.html">regular expression</A>
    of which URIs should be included in the traversal
</DL>
<H3>
  <A NAME="Search">Breath First or Depth First Search</A>
</H3>
<P>
The webbot can perform either a
<A HREF="http://study.haifa.ac.il/~gmazorvs/">Depth First Search (DFS) or
a Breadth First Search (BFS)</A>. The default is DFS where the robot issues
new requests as soon as they are encountered. To change to the BFS algorithmn,
use the "<TT>-bfs</TT>" flag:
<DL>
  <DT>
    <B>-bfs</B>
  <DD>
    Use Breadth First Search (BFS) instead of Depth First Search (DFS)
</DL>
<H3>
  <A NAME="Inlined">Checking Inlined Images</A>
</H3>
<P>
The webbot can check inlined images as well as normal hyperlinks. You can
control this using the following flags:
<DL>
  <DT>
    <B>-img</B>
  <DD>
    Test include <STRONG>inlined images </STRONG>using a <TT>HEAD</TT> request
  <DT>
    <B>-saveimg</B>
  <DD>
    Saving the <STRONG>inlined images</STRONG> on local disk or pump them to
    a black hole. This is primarily to emulate a GUI client's behavior using
    the robot
  <DT>
    <B>-alt [ file ]</B>
  <DD>
    Specifies a
    <A HREF="http://www.apache.org/docs/mod/mod_log_referer.html">Referer Log
    Format</A> style log file of all inlined images <EM>without or with an
    empty</EM> an <STRONG>ALT</STRONG> tag.
  <DT>
    <B>-prefix [ URI ]</B>
  <DD>
    Define a URI prefix for all inlined image URIs - if they do not match the
    prefix then they are not checked. The rejected URIs can be
    <A HREF="#Logging">logged to a separate file</A>.
</DL>
<H3>
  <A NAME="Logging">SQL Based Logging using MySQL</A>
</H3>
<P>
Using SQL based logging requires that you have linked against a
<A href="http://www.tcx.se/">MySQL library</A>. See the
<A href="../../INSTALL.html">installation instructions</A> for details. I
like the Web interface provided by
<A HREF="http://www.daa.com.au/~james/www-sql/">www-sql</A> which makes it
easy to access the logged data. The data is stored in four tables within
the same database (the default name is "<CODE>webbot</CODE>"):
<DL>
  <DT>
    <B>uris</B>
  <DD>
    An index that maps URIs to integers so that they are easier to refer to
  <DT>
    <B>requests</B>
  <DD>
    Contains information from the request including the request-URI, the method,
    and the resulting status code.
  <DT>
    <B>resources</B>
  <DD>
    Contains information of the resource like content-type, content-encoding,
    expires, etc.
  <DT>
    <B>links</B>
  <DD>
    Contains information about which documents point to which dopuments, the
    type of the link etc. The type can either be implicit like "<EM>referer</EM>"
    or "<EM>image</EM>", or it can be explicit like "<EM>stylesheet</EM>",
    "<EM>toc</EM>", etc.
</DL>
<P>
The command line options for handling the SQL logging are as follows:
<DL>
  <DT>
    <B>-sqlserver [ srvrname ]</B>
  <DD>
    Specify the mysql server. The default is <CODE>"localhost</CODE>".
  <DT>
    <B>-sqldb [ dbname ]</B>
  <DD>
    Specify the database to use. The default is <CODE>webbot</CODE>. Note that
    webbot creates its own set of tables for handling the logs.
  <DT>
    <B>-sqluser [ usrname ]</B>
  <DD>
    Use this to specify the user that we are connection to the database as. The
    default is <CODE>"webbot</CODE>".
  <DT>
    <B>-sqlpassword [ usrpswd ]</B>
  <DD>
    Use this to specify the password needed to connect to the database. The default
    is empty string.
  <DT>
    <B>-sqlrelative [ relroot ]</B>
  <DD>
    If you want to make the URI entries in the database relative then you can
    specify the root to which they should be made relative. This can for example
    be used to built the database on another machine than is normally running
    the service. On heavy loaded sites, it is often a good idea to have an internal
    test server running which can be used to build the database as it does take
    some resources.
  <DT>
    <B>-sqlexternals</B>
  <DD>
    Use this flag if you want all links that have been filtered because they
    didn't fulfill the constraints to be logged as well in the same table as
    all other URIs.
  <DT>
    <B>-sqlclearlinks</B>
  <DD>
    Clears the <B>links</B> table before starting the traversal.
  <DT>
    <B>-sqlclearrequests</B>
  <DD>
    Clears the <B>requests</B> table before starting the traversal.
  <DT>
    <B>-sqlclearresources</B>
  <DD>
    Clears the <B>resources</B> table before starting the traversal.
  <DT>
    <B>-sqlclearuris</B>
  <DD>
    Clears the <B>uris</B> table before starting the traversal.
</DL>
<H3>
  <A NAME="Regular">Regular Ascii Text Logging</A>
</H3>
<P>
This set of log files are dumped in normal ASCII format into local files
<DL>
  <DT>
    <B>-404 [ file ]</B>
  <DD>
    Specifies a
    <A HREF="http://www.apache.org/docs/mod/mod_log_referer.html">Referer Log
    Format</A> style log file of all links resulting in a 404 (Not Found) status
    code
  <DT>
    <B>-l [ file ]</B>
  <DD>
    Specifies a
    <A HREF="../../Daemon/User/Config/Logging.html#common-logfile-format">Common
    Log File Format</A> style log file with a list of visited documents and the
    result codes obtained.
  <DT>
    <B>-negotiated [ file ]</B>
  <DD>
    Specifies a log file of all URIs that where subject to content negotiation.
  <DT>
    <B>-referer [ file ]</B>
  <DD>
    Specifies a
    <A HREF="http://www.apache.org/docs/mod/mod_log_referer.html">Referer Log
    Format</A> style log file of which documents points to which documents
  <DT>
    <B>-reject [ file ]</B>
  <DD>
    Specifies a log file of all the URIs encountered that didn't fulfill the
    <A HREF="#Constaints">constraints for traversal</A>.
</DL>
<H3>
  <A NAME="Stats">Distribution and Statistics Features</A>
</H3>
<P>
Note that if you are using SQL based logging then the set of statictics that
can be drawn directly from the database is very high.
<DL>
  <DT>
    <B>-format [ file ]</B>
  <DD>
    Specifies a log file of which <STRONG>media types</STRONG> (content types)
    were encountered in the run and their distribution
  <DT>
    <B>-charset [ file ]</B>
  <DD>
    Specifies a log file of which <STRONG>charsets</STRONG> (content type parameter)
    were encountered in the run and their distribution
  <DT>
    <B>-hitcount [ file ]</B>
  <DD>
    Specifies a log file of URIs sorted after <STRONG>how many times</STRONG>
    they were referenced in the run
  <DT>
    <B>-lm [ file ]</B>
  <DD>
    Specifies a log file of URIs sorted after <STRONG>last modified date</STRONG>.
    This gives a good overview of the dynamics of the web site that you are checking.
  <DT>
    <B>-rellog [ file ]</B>
  <DD>
    Specifies a log file of any link relationship found in the <STRONG>HTML LINK
    tag</STRONG> (either the <STRONG>REL</STRONG> of the <STRONG>REV</STRONG>
    attribute) that has the relation specified in the <B>-relation</B> parameter
    (all relations are modelled by libwww as "forward"). For example <TT>"-rellog
    stylesheets-logfile.txt -relation stylesheet"</TT> will produce a log file
    of all link relationships of type "stylesheet". The format of the log file
    is
    <P ALIGN=Center>
    <TT>"&lt;relationship&gt; &lt;media type&gt; &lt;from-URI&gt; --&gt;
    &lt;to-URI&gt;"</TT>
    <P>
    meaning that the <TT>from-URI</TT> has the forward <TT>relationship</TT>
    with <TT>to-URI</TT>.
  <DT>
    <B>-title [ file ]</B>
  <DD>
    Specifies a log file of URIs sorted after any <STRONG>title</STRONG> found
    either as an HTTP header or in the HTML.
</DL>
<H3>
  <A NAME="Persistent">Persistent Cache</A>
</H3>
<P>
The webbot can use the persistent cache while travesing the web site which
may casue a significant performance optimization. These are the command line
options:
<DL>
  <DT>
    <B>-cache</B>
  <DD>
    Enable the <A HREF="../../Library/src/WWWCache.html">libwww persistent
    cache</A>
  <DT>
    <B>-cacheroot [ dir ]</B>
  <DD>
    Where should the cache be located? The default is <TT>/tmp/w3c-cache</TT>
  <DT>
    <B>-validate</B>
  <DD>
    Force validation using either the <TT>etag</TT> or the <TT>last-modified
    </TT>date provided by the server
  <DT>
    <B>-endvalidate</B>
  <DD>
    Force end-to-end validation by adding a <TT>max-age=0</TT> cache control
    directive
</DL>
<H3>
  <A NAME="Other">Other Options</A>
</H3>
<DL>
  <DT>
    <B>-delay [ n ]</B>
  <DD>
    Specify the write delay in milliseconds for how long we can wait until we
    flush the output buffer when using pipelining. The default value is 50 ms.
    The longer delay, the bigger TCP packets but also longer response time.
  <DT>
    <B>-n</B>
  <DD>
    Non-interactive mode.
  <DT>
    <B>-nopipe</B>
  <DD>
    Do <I>not</I> use HTTP/1.1 pipelining. The default for this option can be
    set using the <A HREF="../../INSTALL.html">configure script under
    installation</A>.
  <DT>
    <B>-o [ file ]</B>
  <DD>
    Redirects output to specified file. This mode forced non-interactive mode.
  <DT>
    <B>-q</B>
  <DD>
    Somewhat quiet mode.
  <DT>
    <B>-Q</B>
  <DD>
    Really quiet mode.
  <DT>
    <B>-r [ file ];</B>
  <DD>
    <A HREF="../../Library/User/Using/Rules.html">Rule file, a.k.a. configuration
    file</A>. If this is specified, a rule file may be used to map URLs, and
    to set up other aspects of the behavior of the browser. Many rule files may
    be given with successive -r options, and a default rule file name may be
    given using the <B>WWW_CONFIG</B> environment variable.
  <DT>
    <A NAME="single"><B>-single</B></A>
  <DD>
    Single threaded mode. If this flag is set then the browser uses blocking,
    non interruptible I/O in interactive mode. Non-interactive mode always uses
    blocking I/O.
  <DT>
    <B>-ss</B>
  <DD>
    Print out date and time for start and stop for the job.
  <DT>
    <B>-timeout [ n ]</B>
  <DD>
    Timeout in seconds on open connections. If we don't get a reply within n
    secs then about the request. Default timeout is 20 secs.
</DL>
<H2>
  <A NAME="z16">URI</A>
</H2>
<P>
The URI is the <A HREF="../../Addressing/Addressing.html"> hypertext
address</A> of the document at which you want to start the robot.
<H2>
  <A NAME="z17">keywords</A>
</H2>
<P>
Any further command line arguments are taken as keywords. Keywords can be
used as search tokens in an HTTP request-URI encoded so that all spaces are
replaced with "<TT>+</TT>" and unsafe characters are encoded using the URI
"<TT>%xx</TT>" escape mechanism. An example of a search query is
<PRE>	webbot http://... "RECORD=ID" "par1=a" "par2=b" "par3=c" "par4=d"
</PRE>
<P>
  <HR>
<ADDRESS>
  <A HREF="../../People/Frystyk/">Henrik Frystyk Nielsen</A><BR>
  @(#) $Id$
</ADDRESS>
</BODY></HTML>
